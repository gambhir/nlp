# -*- coding: utf-8 -*-
"""Copy of Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YygF_4F9m6doTIn1uKKczqu-IFTTlnH-
"""

import keras
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelBinarizer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Activation, Dense, Dropout
from keras.preprocessing.text import Tokenizer
import json

num_labels = 3
vocab_size = 10000
batch_size = 30
epochs = 5

x = []
y = []

with open('/content/sample_data/annotation-single.json') as f:
    data = json.load(f)

for labels in data:
    #print(labels['attributes']['scene'])
    text = ''
    y.append(labels['attributes']['scene'])
    text += labels['attributes']['weather']
    text += ' '
    #text += labels['attributes']['timeofday']
    #text += ' '
    for label in labels['labels']:
        text += label['category'] 
        text += ' '
        if label['manualShape'] is True:
            text += "manualShape"     
            text += ' '
        
        if "trafficLightColor" in label['attributes']:
            text += label['attributes']['trafficLightColor']
            text += ' '
        if "areaType" in label['attributes']:  
            text += label['attributes']['areaType']
            text += ' '
    x.append(text)

#print(x)
#print(y)     

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)


#print(x_train[0])
#print(x_test[0])  
 
# define Tokenizer with Vocab Size
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(x_train)

#x_train = tokenizer.texts_to_matrix(x_train, mode='count') #got low accuracy with mode='count'
#x_test = tokenizer.texts_to_matrix(x_test, mode='count')

#x_train = tokenizer.texts_to_matrix(x_train, mode='freq')
#x_test = tokenizer.texts_to_matrix(x_test, mode='freq')


x_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')
x_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')

#freq
 
#print(x_test)

encoder = LabelBinarizer()
encoder.fit(y_train)
y_train = encoder.transform(y_train)
y_test = encoder.transform(y_test)

model = Sequential()
model.add(Dense(512, input_shape=(vocab_size,)))
model.add(Activation('relu')) #relu better than sigmoid  
model.add(Dropout(0.3))
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(num_labels))
model.add(Activation('softmax'))
model.summary()
#model.compile(loss='Poisson', optimizer='adam', metrics=['accuracy'])
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #categorical_crossentropy slightly better performance
 
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_split=0.1)

score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)

print('Test loss:', score[0])
print('Test accuracy:', score[1])

#